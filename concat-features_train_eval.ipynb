{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./features/concat_test2_en.tsv\n",
      "./features/concat_test2_fr.tsv\n",
      "./features/concat_test2_yt.tsv\n",
      "./features/concat_test_en.tsv\n",
      "./features/concat_test_fr.tsv\n",
      "./features/concat_test_yt.tsv\n",
      "./features/concat_train_en.tsv\n",
      "./features/concat_train_fr.tsv\n",
      "./features/concat_train_yt.tsv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "FEATURES_PATH=./features\n",
    "\n",
    "paste \"$FEATURES_PATH\"/train/fr/*.tsv > \"$FEATURES_PATH\"/concat_train_fr.tsv\n",
    "paste \"$FEATURES_PATH\"/train/en/*.tsv > \"$FEATURES_PATH\"/concat_train_en.tsv\n",
    "paste \"$FEATURES_PATH\"/train/yt/*.tsv > \"$FEATURES_PATH\"/concat_train_yt.tsv\n",
    "paste \"$FEATURES_PATH\"/test/fr/*.tsv  > \"$FEATURES_PATH\"/concat_test_fr.tsv\n",
    "paste \"$FEATURES_PATH\"/test/en/*.tsv  > \"$FEATURES_PATH\"/concat_test_en.tsv\n",
    "paste \"$FEATURES_PATH\"/test/yt/*.tsv  > \"$FEATURES_PATH\"/concat_test_yt.tsv\n",
    "paste \"$FEATURES_PATH\"/test2/fr/*.tsv > \"$FEATURES_PATH\"/concat_test2_fr.tsv\n",
    "paste \"$FEATURES_PATH\"/test2/en/*.tsv > \"$FEATURES_PATH\"/concat_test2_en.tsv\n",
    "paste \"$FEATURES_PATH\"/test2/yt/*.tsv > \"$FEATURES_PATH\"/concat_test2_yt.tsv\n",
    "\n",
    "ls \"$FEATURES_PATH\"/*tsv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_yt ['partage', 'sim_texte_tittre', 'nb_fakewords', 'ponctuation', 'proba2', 'proba1', 'sim_tfidf']\n",
      "train_fr ['partage', 'sim_texte_tittre', 'nb_fakewords', 'ponctuation', 'topic_title_sim', 'proba2', 'proba1', 'sim_tfidf']\n",
      "train_en ['proba3', 'partage', 'sim_texte_tittre', 'nb_fakewords', 'ponctuation', 'topic_title_sim', 'proba2', 'proba1', 'sim_tfidf']\n",
      "test2_en ['proba3', 'partage', 'sim_texte_tittre', 'nb_fakewords', 'ponctuation', 'topic_title_sim', 'proba2', 'proba1', 'sim_tfidf']\n",
      "test2_fr ['partage', 'sim_texte_tittre', 'nb_fakewords', 'ponctuation', 'topic_title_sim', 'proba2', 'proba1', 'sim_tfidf']\n",
      "test2_yt ['partage', 'sim_texte_tittre', 'nb_fakewords', 'ponctuation', 'proba2', 'proba1', 'sim_tfidf']\n"
     ]
    }
   ],
   "source": [
    "FEATURES_PATH = \"./features\"\n",
    "paths = [\"train_yt\",  \"train_fr\", \"train_en\",\n",
    "         #\"concat_test_en.tsv\", \"concat_test_fr.tsv\", \"concat_test_yt.tsv\"\n",
    "         \"test2_en\", \"test2_fr\", \"test2_yt\"] \n",
    "\n",
    "\n",
    "X, y = {}, {}\n",
    "\n",
    "models = {}\n",
    "\n",
    "features = set('id partage nb_fakewords proba1 proba2 proba3 sim_tfidf sim_texte_tittre ponctuation'.split())\n",
    "features = set('id proba1 proba2 proba3'.split())\n",
    "features = set('id proba1 proba2 proba3 partage nb_fakewords sim_tfidf sim_texte_tittre ponctuation topic_title_sim'.split())\n",
    "\n",
    "\n",
    "for name in paths:\n",
    "    df = pd.read_csv(f'{FEATURES_PATH}/concat_{name}.tsv', sep=\"\\t\")\n",
    "    df[\"id\"] = df[\"id\"].astype(np.int32)\n",
    "    df.set_index('id', inplace=True)\n",
    "    df = df.fillna(-1)\n",
    "    df[\"ponctuation\"] = df[\"ponctuation\"].astype(np.int32) # apply(lambda l: float(l)) #\n",
    "    \n",
    "    cols = list(features & set(df.columns.tolist()))\n",
    "    print(name, cols) # , df.columns) \n",
    "    X[name] = df[cols]\n",
    "    X[name].to_csv(f\"{FEATURES_PATH}/normalized/{name}.tsv\", sep=\"\\t\")\n",
    "\n",
    "    if \"label\" in df.columns:\n",
    "        y[name] = df.label\n",
    "        m = LogisticRegression()\n",
    "        ## Normalisation des données\n",
    "        pipeline = make_pipeline(StandardScaler(), m)\n",
    "        models[name] = pipeline.fit(X[name], y[name])\n",
    "        # print(dict(zip(cols, m.feature_importances_)))\n",
    "    else:\n",
    "        y[name] = models[name.replace(\"test2\", \"train\")].predict(X[name])\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_predictions(predictions):\n",
    "    for name, preds in predictions.items():\n",
    "        if \"test2_\" in name:\n",
    "            source = name.replace(\"test2_\",\"\")\n",
    "            for doc_id, pred  in zip(X[name].index, preds):\n",
    "                yield (int(doc_id), pred, source)\n",
    "\n",
    "results = pd.DataFrame(gen_predictions(y), columns=['id', 'type_pred', 'source'])\n",
    "results.set_index(\"id\", inplace=True)\n",
    "results.to_csv(\"./test2-results_full_features_concat.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_pred</th>\n",
       "      <th>score</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>trusted</td>\n",
       "      <td>0.877590</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>fakeNews</td>\n",
       "      <td>0.556900</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3914</th>\n",
       "      <td>fakeNews</td>\n",
       "      <td>0.884177</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>fakeNews</td>\n",
       "      <td>0.565416</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>fakeNews</td>\n",
       "      <td>0.914755</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type_pred     score source\n",
       "id                             \n",
       "4274   trusted  0.877590     en\n",
       "1022  fakeNews  0.556900     en\n",
       "3914  fakeNews  0.884177     en\n",
       "3470  fakeNews  0.565416     en\n",
       "2004  fakeNews  0.914755     en"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fasttext = pd.read_csv(f\"./test2-results-prob_fasttext.tsv\", sep=\"\\t\")\n",
    "results_fasttext.set_index(\"id\", inplace=True)\n",
    "results_fasttext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr fasttext\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   fakeNews       0.00      0.00      0.00        94\n",
      "    trusted       0.60      1.00      0.75       142\n",
      "\n",
      "avg / total       0.36      0.60      0.45       236\n",
      "\n",
      "fr features + fasttext\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   fakeNews       0.35      0.40      0.37        94\n",
      "    trusted       0.56      0.49      0.52       142\n",
      "\n",
      "avg / total       0.47      0.46      0.46       236\n",
      "\n",
      "en fasttext\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   fakeNews       0.84      0.69      0.76       459\n",
      "     satire       0.00      0.00      0.00        17\n",
      "    trusted       0.83      0.93      0.88       801\n",
      "\n",
      "avg / total       0.82      0.83      0.83      1277\n",
      "\n",
      "en features + fasttext\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   fakeNews       0.37      0.38      0.38       459\n",
      "     satire       0.08      0.06      0.07        17\n",
      "    trusted       0.63      0.63      0.63       801\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1277\n",
      "\n",
      "yt fasttext\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   fakeNews       0.87      0.96      0.91        68\n",
      "    trusted       0.00      0.00      0.00        10\n",
      "\n",
      "avg / total       0.76      0.83      0.79        78\n",
      "\n",
      "yt features + fasttext\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   fakeNews       0.87      0.88      0.88        68\n",
      "    trusted       0.11      0.10      0.11        10\n",
      "\n",
      "avg / total       0.77      0.78      0.78        78\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/bin/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "test_data = {}\n",
    "for name in [\"fr\", \"en\", \"yt\"]:\n",
    "\n",
    "    test_data[name] = pd.read_csv(f\"../data/test2-full/storyzy_{name}_test2_full.tsv\", sep=\"\\t\")\n",
    "    test_data[name].set_index('id', inplace=True)\n",
    "    test_data[name][\"y_pred\"] = test_data[name].join(results[results.source==name])[\"type_pred\"]\n",
    "    test_data[name][\"y_pred_fasttext\"] = test_data[name].join(results_fasttext[results_fasttext.source==name])[\"type_pred\"]\n",
    "\n",
    "    print(name, 'fasttext')\n",
    "    print(classification_report(test_data[name]['type'], test_data[name]['y_pred_fasttext']))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(name, 'features + fasttext')\n",
    "    print(classification_report(test_data[name]['type'], test_data[name]['y_pred']))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
